+++
+++
# 文章大纲

#### **Intro**

1. 云环境选择使用TM降本扩容
   1. 虚拟机是云架构的基础
   2. 内存发展落后于core增长, 限制了云scalability
   3. TM的定义b2
   4. 使用TM扩容需要降低SMEM的开销
   5. 利用data placement是核心方法

#### **Background and Motivation 2000W 2F V0.2**

1. hypervisor假设OS没有TM管理能力
   1. HeteroVisor
      1. 以CPU的p-state作为灵感, 将混合内存以e-state形式包装给guest.
      2. guest通过调节的e-state数值向hypervisor要求提高内存的性能.
      3. hypervisor会完全透明的将guest的物理页更多的映射到FMEM.
   2. HeteroOS
      1. 尽管引入了guest OS awareness.
      2. 还是认为guest OS缺少硬件权限做hotness tracking.
      3. 于是将TM管理交给hypervisor.
   3. vTMM 及 Memstrata
      1. 均采用hypervisor only的TM管理.
      2. vTMM由hypervisor全权负责sampling/classification/migration.
      3. Memstrata则完全放弃了软件TM的placement管理, hyperisor仅管理VM分配到的FMEM/SMEM数量.
2. 现有OS对TM支持很普遍 => 设计hballoon将OS的TM管理引入虚拟化环境
   1. 尽管AutoNUMA很早就支持以NUMA形式暴露TM.
      1. 以NUMA形式管理TM, promote on hint fault
      2. 问题1: sampling (hint fault) 以及classification (split-LRU traversing) 都开销巨大
      3. 问题2: 仅在访问时迁移
   2. 过渡段
      1. AutoNUMA落后的data placement管理让hypervisor TM走上了hypervisor only的道路
      2. 但最近五年OS TM的爆发使得我们应重新审视这条道路.
   3. Nimble/Autotiering/TPP/Nomad后续继续改进
      1. Nimble支持巨页迁移/Autotiering支持多层级且介质优先的迁移/TPP支持进程调度时按内存压力触发迁移/Nomad支持异步迁移
      2. 均没有摆脱sampling以及classification的巨大开销
   4. Memtis尝试变革sampling
      1. 引入硬件加速的sampling方法
      2. classification的开销仍没有解决
3. 云TM需要最小CPU开销 => 设计HPlacement优化OS的TM管理
   1. CPU利用率直接影响到成本和利润
   2. Hypervisor-based开销和安全性成问题.
      1. 需要额外的地址转换来管理hotness
      2. para-virt选择通过page fault来mirror pgtbl
      3. hw-virt选择通过监控pgtbl地址
   3. 现有Linux-derived的设计sampling及classification均开销巨大 (加图: cycle per sample)
      1. sampling
         1. pgtbl scan
      2. classification
         1. LRU rotate
      3. migration
         1. hint fault
   4. IS准确且开销低且通用且隐私好
   5. Memtis的Cold detection很差
4. guest IS: 硬件支持且有privacy保证
   1. IS如何工作
   2. Intel pebs guest的支持
      1. isolation很好

#### **Background and Motivation 2000W 2F V0.3**

1. TMM methodology
   1. (introduce by methodology; 3.b)
   2. 总体分三步
      1. sampling
         1. pgtbl
         2. pebs
      2. classification
         1. lru rotating
         2.
      3. migration
         1. hint fault triggered promotion
         2. memory shortage triggered demotion
         3.
   3. 现有两个流派
      1. pagetable centric
      2. sampling based
2. hypervisor假设OS没有TM管理能力
   1. (comparison table)
   2. HeteroVisor
      1. 以CPU的p-state作为灵感, 将混合内存以e-state形式包装给guest.
      2. guest通过调节的e-state数值向hypervisor要求提高内存的性能.
      3. hypervisor会完全透明的将guest的物理页更多的映射到FMEM.
   3. HeteroOS
      1. 尽管引入了guest OS awareness.
      2. 还是认为guest OS缺少硬件权限做hotness tracking.
      3. 于是将TM管理交给hypervisor.
   4. vTMM 及 Memstrata
      1. 均采用hypervisor only的TM管理.
      2. vTMM由hypervisor全权负责sampling/classification/migration.
      3. Memstrata则完全放弃了软件TM的placement管理, hyperisor仅管理VM分配到的FMEM/SMEM数量.
   5.

|             | guest mgmt ability | guest awareness | hotness mgmt | locality visibility | isolation | access sampling   | composition | elasticity |
| ----------- | ------------------ | --------------- | ------------ | ------------------- | --------- | ----------------- | ----------- | ---------- |
| HeteroVisor | x                  | x               | √            | x                   | x         | GPT mirror + scan |             | x          |
| HeteroOS    | x                  | √               | √            | x                   | x         | GPT mirror + scan | √           | x          |
| RAMinate    | x                  | x               | √            | x                   | x         | EPT scan          |             | x          |
| vTMM        | x                  | x               | √            | x                   | x         | GPT scan          | √           | x          |
| Memstrata   | x                  | x               | x            | x                   | √         | x                 |             |            |
| TMDR        | √                  | √               | √            | √                   | √         | PEBS              |             |            |

1. motivation
   1. challenge 1: TM provision
      1. TM awareness
      2. elasticity
      3. isolation
   2. challenge 2: overhead control
      1. (exprimental proof)

vmnum       1       2       3       4       6       9tpp   4.081361e+11 4.246558e+11 4.909864e+11 6.024525e+11 8.541470e+11 3.660464e+12memtis 1.304787e+11 1.872980e+11 2.694814e+11 3.535079e+11 5.030107e+11 8.060831e+11

1. multi-VM scalability under scarce resource
2. sampling overshoot
3. locality-agostic classification
4. tlb-miss inducing migration

#### **Design 3000W 3F**

1. Overview
   1. 设计HBalloon将OS的TM管理引入虚拟化环境
   2. 设计HPlacement优化OS的TM管理
2. HBalloon: 转化OS TM到云TM
   1. 采用NUMA node的形式暴露TM
   2. 细粒度动态内存的分配与回收
3. overhead constrained gOS hotness management
   1. 事件触发架构
   2. 低开销的sampling
   3. 动态低开销的segment tree classification
   4. page exchange
4. HPlacement
   1. Range-based management
      1. Locality

#### **Implementation 400W**

1. Base system and version
2. LoC stats
3. How PMEM is used

#### **Evaluation 3000W 10F**

1. 主要目标
2. 实验环境
3. HB+gOS的可行性+兼容性实验
   1. HB 对比 VB (X=vmnum Y=gups C=balloon)    **1 √**
   2. guest内存组成对比
   3. HB的resize性能 (X=time Y=size C=balloon)    **?**
4. 与多gOS对比: HA能更高效的利用FMEM
   1. 多VM下多scalability (X=vmnum Y=gups C=os)                  **2 √**
   2. (hotness detection)时序图对比反应速度 (X=time Y=gups C=os)  **2 √**
   3. 开销breakdown (X=stage Y=cycle C=os)                        **2**
      1. 总之就是一张三段breakdown对比图另外再加一张每段的breakdowndown
      2. sampling可以对比hint fault以及收到sample的总数
      3. classification可以lru sanning以及其他相关数据结构维护所花的时间
      4. migration可以对比
   4. paired migration对比exchange (X=method Y=throughput)      **?**
   5. dynamic hotset: 还是看realtime gups不过gups改成后面iteration要shift一下hotset
5. 参数的敏感性? gups
   1. 不同的DRAM比例
   2. 不同的event period
   3. 不同的hot thresh
   4. 不同的access pattern
6. HB+HA的通用性
   1. realworld workload (X=workload Y=speedup C=os)             **?**
      1. 详见Memtis Table 2. Benchmark characteristics.
      2. graph500 **√x 比tpp略差**
      3. pagerank **√x 比tpp略差**
      4. xsbench **√**
      5. liblinear **√**
      6. silo **ycsb√  tpcc比memtis略差**
      7. btree **√x 比memtis略差**
      8. 603.bwaves **√x 比tpp略差**
      9. 654.roms x 比tpp/nomad都差
7. CXL version

#### **Discussion 1000W**

*这里应该放第二章没有串起来但是又比较重要的文章.*

1. Hardware tiering/caching:
   1. PMEM Memory Mode
   2. Flat Memory Mode: 仍然属于SMEM的范畴.
2. hotplug and virtio-mem s1
   1. hotplug最小128MiB的粗粒度
   2. 均需要空出连续的物理内存
3. VM内存的动态平衡.
4. PML logging
5. guest instruction sampling
6. Para-virtualization (HeteroVisor) and hardware-virtualization
7. Colloid
   1. placement storm
      1. 因为不同VM share同一套uncore CHA, 一个VM的placement决策会由CHA传导到所有VM, 导致placement storm以及bouncing
   2. isolation/security
      1. hypevisor可以虚拟化CPU core以及per core PMU, 但是无法虚拟化shared uncore CHA hardware
      2. 共享uncore CHA可能引发side channel attack
   3. 开销
      1. Colloid可能(Colloid-TPP)需要一个单独的core做spin polling.
8. ballooning
   1. HeteroOS: sec4.2 balloon swap pages disk

# 实验结果

Motivation2实验

```
//        | Samping                | Classification                           | Migration
// -------+------------------------+------------------------------------------+--------------------------
// TPP    | PTEA_SCAN_NS           | LRU_ROTATE_NS - PTEA_SCAN_NS - DEMOTE_NS | DEMOTE_NS + HINT_FAULT_NS
// Nomad  | PTEA_SCAN_NS           | LRU_ROTATE_NS - PTEA_SCAN_NS - DEMOTE_NS | DEMOTE_NS + HINT_FAULT_NS
// Memtis | SAMPLING_NS - PTEXT_NS | LRU_ROTATE_NS + PTEXT_NS                 | DEMOTE_NS + PROMOTE_NS
```

TPP/Memtis总开销对比. 性能上看除了vmnum==1我们还是最好link.

```
out = pd.DataFrame()
for kernel, metrics in dict(
        tpp=["lru_rotate_ns", "hint_fault_ns"],
        memtis=["sampling_ns", "lru_rotate_ns", "demote_ns", "promote_ns"],
).items():
    out[kernel] = df.query(f"kernel == '5.15.162-{kernel}+' ").loc[:, ("vmnum", *metrics)].groupby("vmnum").sum().T.sum()
print(out.T.to_csv())
vmnum              1             2             3             4             6             9
tpp     4.081361e+11  4.246558e+11  4.909864e+11  6.024525e+11  8.541470e+11  3.660464e+12
memtis  1.304787e+11  1.872980e+11  2.694814e+11  3.535079e+11  5.030107e+11  8.060831e+11
```

# 其他

graph500行为分析
我们使用opemmp版本, 二进制为omp-csr. 线程数通过OMP_NUM_THREADS=控制. 不能设置SKIP_VALIDATION=, 否则无法报告最终的性能统计情况.
scale=24且edge=24时内存占用为12,539MiB. 内存占用随scale指数增大, 随edge线性增大.
我们修改了options.c支持通过-n传入iteration数量, 默认为64.
物理机上纯DRAM运行32个iteration耗时 5:27.86.
单个VM中20% DRAM运行64iteration耗时12:49.23/20:47.05/18:30.36/13:34.52
最终性能harmonic_mean_TEPS: 2.95520729149981499e+09 harmonic_stddev_TEPS: 7.05650441698903799e+08
程序在main通过make_graph.c中的make_graph()初始化edgelist, 长度为(2^s) *e, 之后开始循环iteration, bfs遍历生成的图.
潜在热点的vertices相关数据结构有两个.
其中一个地址存在xoff中, 会在全部iteration开始前分配, 结束后释放, 长度为2*nv+2. 分配函数为alloc_graph(), 释放函数为free_graph().
另一个为bfs_tree, 会在每次iteration开始时分配, 结束后释放.
最终分配函数并非直接调用mmap, 而是用了malloc()的wrapper, 即实现在utils.c中的xmalloc(). 我们可以修改这里, 即加memset,使得虚拟内存分配一定会match到物理页.
不难发现graph500正好会先将cold data即edgelist放入DRAM, 后将hotdata即vertex相关数据放入PMEM.
在没有设置SEED环境变量的情况下graph500默认使用固定的seed=0xDECAFBAD

```
Options:
  v   : version
  h|? : this message
  R   : use R-MAT from SSCA2 (default: use Kronecker generator)
  s   : R-MAT scale (default 14)
  e   : R-MAT edge factor (default 16)
  A|a : R-MAT A (default 0.57) >= 0
  B|b : R-MAT B (default 0.19) >= 0
  C|c : R-MAT C (default 0.19) >= 0
  D|d : R-MAT D (default 0.05) >= 0
        Note: Setting 3 of A,B,C,D requires the arguments to sum to
        at most 1.  Otherwise, the parameters are added and normalized
        so that the sum is 1.
  V   : Enable extra (Verbose) output
  o   : Read the edge list from (or dump to) the named file
  r   : Read the BFS roots from (or dump to) the named file
  n   : Run NBFS iterations
```

pagerank行为分析
同样基于openmp, 二进制为pr.
默认使用twitter graph, 内存占用为12,548MiB. 内存占用会随iteration次数设置稍有变化, 但不重要.
Graph has 61578415 nodes and 1468364884 directed edges for degree: 23
运行一个trial, 其中包括默认的20次iteration耗时为0:37.04. 预计8个trial足够体现出差异.
pagerank不同于graph500有提供最终的TEPS吞吐分析, 我们可以直接使用耗时作为对比指标.

```
pagerank
 -h          : print this help message
 -f <file>   : load graph from file
 -s          : symmetrize input edge list                               [false]
 -g <scale>  : generate 2^scale kronecker graph
 -u <scale>  : generate 2^scale uniform-random graph
 -k <degree> : average degree for synthetic graph                          [16]
 -m          : reduces memory usage during graph building               [false]
 -a          : output analysis of last run                              [false]
 -n <n>      : perform n trials                                            [16]
 -r <node>   : start from node r                                         [rand]
 -v          : verify the output of each run                            [false]
 -l          : log performance within each trial                        [false]
 -i <i>      : perform at most i iterations                                [20]
 -t <t>      : use tolerance t                                       [0.000100]
```

